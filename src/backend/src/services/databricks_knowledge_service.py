"""
Databricks Knowledge Source Service
"""
from typing import Dict, Any, List, Optional
from fastapi import UploadFile
import logging
import os
import io
import asyncio
import aiohttp
import base64
import json
import uuid
from datetime import datetime
try:
    from databricks.sdk.service.files import FileInfo
except ImportError:
    # Databricks SDK not installed, will use REST API
    FileInfo = None

from sqlalchemy.ext.asyncio import AsyncSession
from src.repositories.databricks_config_repository import DatabricksConfigRepository
from src.utils.databricks_auth import get_workspace_client

logger = logging.getLogger(__name__)


class DatabricksKnowledgeService:
    """Service for managing knowledge files in Databricks Volumes."""

    def __init__(self, session: AsyncSession, group_id: str, created_by_email: Optional[str] = None):
        """
        Initialize the Databricks Knowledge Service.

        Args:
            session: Database session
            group_id: Group ID for tenant isolation
            created_by_email: Email of the user
        """
        self.session = session
        self.repository = DatabricksConfigRepository(session)
        self.group_id = group_id
        self.created_by_email = created_by_email
    
    async def upload_knowledge_file(
        self,
        file: UploadFile,
        execution_id: str,
        group_id: str,
        volume_config: Dict[str, Any],
        agent_ids: Optional[List[str]] = None,
        user_token: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Upload a file to Databricks Volume for knowledge source.

        Args:
            file: The uploaded file
            execution_id: Execution ID for scoping
            group_id: Group ID for tenant isolation
            volume_config: Volume configuration
            agent_ids: Optional list of agent IDs that can access this knowledge source
            user_token: Optional user token for OBO authentication

        Returns:
            Upload response with file path and metadata
        """
        import base64
        import aiohttp

        logger.info("="*60)
        logger.info("STARTING KNOWLEDGE FILE UPLOAD")

        # CRITICAL DEBUG: Log what agent_ids we receive in the service
        logger.info(f"[SERVICE] ðŸ” AGENT_IDS RECEIVED: {agent_ids} (type: {type(agent_ids)}, length: {len(agent_ids) if agent_ids else 0})")

        if agent_ids:
            logger.info(f"[SERVICE] âœ… Agent IDs detected: {agent_ids}")
        else:
            logger.warning(f"[SERVICE] âš ï¸ No agent_ids provided - this will result in null agent_ids in vector index!")

        logger.info(f"File: {file.filename}")
        logger.info(f"Execution ID: {execution_id}")
        logger.info(f"Group ID: {group_id}")
        logger.info(f"Volume Config: {volume_config}")
        logger.info("="*60)

        try:
            # Get Databricks configuration
            config = await self.repository.get_active_config(group_id=group_id)
            if not config:
                # Use default configuration if none exists - get from unified auth
                logger.warning("No Databricks config found in database, using unified auth defaults")
                workspace_url = 'https://example.databricks.com'
                token = ''
                try:
                    from src.utils.databricks_auth import get_auth_context
                    auth = await get_auth_context()
                    if auth:
                        workspace_url = auth.workspace_url
                        token = auth.token
                        logger.debug(f"Using unified {auth.auth_method} auth for default config")
                except Exception as e:
                    logger.warning(f"Failed to get unified auth for default config: {e}")

                config = type('obj', (object,), {
                    'knowledge_volume_enabled': True,
                    'knowledge_volume_path': 'main.default.knowledge',
                    'workspace_url': workspace_url,
                    'encrypted_personal_access_token': token
                })()
                logger.info(f"Default config - workspace_url: {config.workspace_url}")
                logger.info(f"Default config - volume_path: {config.knowledge_volume_path}")
            else:
                logger.info(f"Found Databricks config - workspace_url: {config.workspace_url}")
                logger.info(f"Found Databricks config - volume_enabled: {config.knowledge_volume_enabled}")
                logger.info(f"Found Databricks config - volume_path: {config.knowledge_volume_path}")

            # Construct volume path
            volume_path = volume_config.get('volume_path', 'main.default.knowledge')
            if hasattr(config, 'knowledge_volume_path') and config.knowledge_volume_path:
                volume_path = config.knowledge_volume_path

            logger.info(f"Using volume path: {volume_path}")

            # Parse volume path (format: catalog.schema.volume)
            parts = volume_path.split('.')
            if len(parts) != 3:
                # Use defaults if invalid format
                catalog, schema, volume = 'main', 'default', 'knowledge'
                logger.warning(f"Invalid volume path format '{volume_path}', using defaults: {catalog}.{schema}.{volume}")
            else:
                catalog, schema, volume = parts
                logger.info(f"Parsed volume path - Catalog: {catalog}, Schema: {schema}, Volume: {volume}")

            # Create date directory if configured
            date_dir = ""
            if volume_config.get('create_date_dirs', True):
                date_dir = datetime.now().strftime('%Y-%m-%d')
                logger.info(f"Creating date directory: {date_dir}")

            # Construct full path - ensure it starts with /Volumes
            # Don't add 'knowledge' subdirectory since the volume itself is already 'knowledge'
            file_path = f"/Volumes/{catalog}/{schema}/{volume}/{group_id}/{execution_id}"
            if date_dir:
                file_path = f"{file_path}/{date_dir}"
            file_path = f"{file_path}/{file.filename}"
            file_path = file_path.replace('//', '/')  # Clean up double slashes

            logger.info("="*60)
            logger.info(f"FULL UPLOAD PATH: {file_path}")
            logger.info("="*60)

            # Read file content
            content = await file.read()
            file_size = len(content)
            logger.info(f"File size: {file_size} bytes ({file_size/1024:.2f} KB)")

            # Get Databricks token using unified authentication system
            from src.utils.databricks_auth import get_auth_context

            # Get workspace URL from config or unified auth
            workspace_url = getattr(config, 'workspace_url', None)
            if not workspace_url:
                auth = await get_auth_context()
                if auth:
                    workspace_url = auth.workspace_url
                    logger.debug(f"Using workspace URL from unified {auth.auth_method} auth")
            logger.info(f"Workspace URL: {workspace_url}")

            try:
                # Unified auth handles: OBO, OAuth, PAT from database, PAT from environment
                auth = await get_auth_context(user_token=user_token)
                if not auth:
                    raise Exception("Failed to get authentication context")

                token = auth.token
                workspace_url = auth.workspace_url
                logger.info(f"Successfully obtained Databricks authentication: {auth.auth_method}")

                if client_id and client_secret:
                    logger.info("Attempting OAuth authentication with client credentials")
                    try:
                        from databricks.sdk.core import Config
                        oauth_config = Config(
                            host=workspace_url,
                            client_id=client_id,
                            client_secret=client_secret
                        )
                        # Get OAuth token
                        auth_result = oauth_config.authenticate()
                        if hasattr(auth_result, 'access_token'):
                            token = auth_result.access_token
                            logger.info("Successfully obtained OAuth token")
                        else:
                            logger.error("No access token in OAuth response")
                            token = None
                    except Exception as oauth_error:
                        logger.error(f"OAuth authentication failed: {oauth_error}")
                        token = None
                else:
                    logger.debug("No OAuth credentials available")
                    token = None
            except Exception as auth_error:
                logger.error(f"Authentication failed: {auth_error}")
                token = None
                workspace_url = None

            if token:
                logger.info("Databricks token found (length: %d)", len(token))
            else:
                logger.warning("No Databricks token found - will simulate upload")

            # Track upload success and method used
            upload_successful = False
            upload_method = "none"
            selected_agents = volume_config.get('selected_agents', [])

            # If we have a token and workspace URL, try actual upload
            if token and workspace_url and workspace_url != 'https://example.databricks.com':
                logger.info("Attempting REAL upload to Databricks")

                try:
                    # Try using Databricks SDK via centralized auth
                    workspace_client = await get_workspace_client(user_token=token)
                    if workspace_client:
                        logger.info("Using WorkspaceClient from databricks_auth middleware")

                        # Upload using SDK - same as DatabricksVolumeCallback
                        try:
                            logger.info(f"Calling workspace_client.files.upload()")
                            logger.info(f"  - file_path: {file_path}")
                            logger.info(f"  - content size: {len(content)} bytes")
                            logger.info(f"  - overwrite: True")

                            # Wrap bytes in BytesIO to create a file-like object
                            content_stream = io.BytesIO(content) if isinstance(content, bytes) else content
                            workspace_client.files.upload(
                                file_path=file_path,
                                content=content_stream,
                                overwrite=True
                            )

                            logger.info("="*60)
                            logger.info("SUCCESS! File uploaded via SDK to Databricks")
                            logger.info(f"File location: {file_path}")
                            logger.info("You should see this file in your Databricks workspace at:")
                            logger.info(f"  {workspace_url}/browse/files{file_path}")
                            logger.info("="*60)

                            upload_successful = True
                            upload_method = "sdk"

                        except Exception as sdk_error:
                            logger.error(f"SDK upload failed: {sdk_error}")
                            logger.info("Falling back to DBFS API method")
                    else:
                        logger.warning("Databricks SDK not available, using DBFS API")

                    # Fallback: Use DBFS API if SDK failed or not available
                    if not upload_successful:
                        logger.info("Attempting upload via DBFS API")
                        async with aiohttp.ClientSession() as session:
                            # Convert content to base64
                            content_b64 = base64.b64encode(content).decode('utf-8')
                            logger.info(f"Encoded content to base64 (length: {len(content_b64)})")

                            headers = {
                                "Authorization": f"Bearer {token}",
                                "Content-Type": "application/json"
                            }

                            # Step 1: Create file handle
                            create_url = f"{workspace_url}/api/2.0/fs/files/create"
                            create_data = {
                                "path": file_path,
                                "overwrite": True
                            }

                            logger.info(f"Step 1: Creating file handle at {create_url}")
                            logger.info(f"  Request data: {create_data}")

                            async with session.put(create_url, json=create_data, headers=headers) as response:
                                if response.status != 200:
                                    error_text = await response.text()
                                    logger.error(f"Failed to create file handle: Status {response.status}")
                                    logger.error(f"Error response: {error_text}")
                                    raise Exception(f"Failed to create file: {error_text}")

                                result = await response.json()
                                handle = result.get("handle")
                                logger.info(f"File handle created successfully: {handle}")

                            # Step 2: Add content block
                            add_block_url = f"{workspace_url}/api/2.0/fs/files/add-block"
                            upload_data = {
                                "handle": handle,
                                "data": content_b64
                            }

                            logger.info(f"Step 2: Uploading content to {add_block_url}")
                            logger.info(f"  Handle: {handle}")

                            async with session.post(add_block_url, json=upload_data, headers=headers) as response:
                                if response.status != 200:
                                    error_text = await response.text()
                                    logger.error(f"Failed to upload content: Status {response.status}")
                                    logger.error(f"Error response: {error_text}")
                                    raise Exception(f"Failed to upload content: {error_text}")

                            logger.info("Content uploaded successfully")

                            # Step 3: Close file
                            close_url = f"{workspace_url}/api/2.0/fs/files/close"
                            close_data = {"handle": handle}

                            logger.info(f"Step 3: Closing file at {close_url}")

                            async with session.post(close_url, json=close_data, headers=headers) as response:
                                if response.status != 200:
                                    error_text = await response.text()
                                    logger.error(f"Failed to close file: Status {response.status}")
                                    logger.error(f"Error response: {error_text}")
                                    raise Exception(f"Failed to close file: {error_text}")

                            logger.info("="*60)
                            logger.info("SUCCESS! File uploaded via DBFS API to Databricks")
                            logger.info(f"File location: {file_path}")
                            logger.info("You should see this file in your Databricks workspace at:")
                            logger.info(f"  {workspace_url}/browse/files{file_path}")
                            logger.info("="*60)

                            upload_successful = True
                            upload_method = "dbfs_api"

                except Exception as e:
                    logger.error(f"Failed to upload to Databricks: {e}", exc_info=True)
                    logger.warning("Falling back to simulation mode")

            # Set simulation mode if upload not successful
            if not upload_successful:
                logger.warning("Missing credentials or using example URL - will simulate upload")
                logger.warning(f"  Token available: {bool(token)}")
                logger.warning(f"  Workspace URL: {workspace_url}")
                logger.warning(f"  Is example URL: {workspace_url == 'https://example.databricks.com'}")

                logger.info("="*60)
                logger.info("SIMULATED UPLOAD (not actually uploaded to Databricks)")
                logger.info(f"Would upload to: {file_path}")
                logger.info(f"File: {file.filename}, Size: {file_size} bytes")
                logger.info("To enable REAL uploads:")
                logger.info("  1. Set DATABRICKS_TOKEN environment variable")
                logger.info("  2. Set DATABRICKS_HOST environment variable")
                logger.info("  3. Or configure in Databricks settings page")
                logger.info("="*60)

                upload_method = "simulated"

            logger.info(f"Selected agents for knowledge access: {selected_agents}")

            # SINGLE EMBEDDING EXECUTION: Process and embed the uploaded file once, regardless of upload method
            logger.info(f"[UPLOAD] Starting single embedding for file: {file_path}")
            logger.info(f"[UPLOAD] Upload method: {upload_method}")
            logger.info(f"[UPLOAD] Embedding params - execution_id: {execution_id}, group_id: {group_id}")

            try:
                embedding_result = await self._embed_uploaded_file(
                    file_path=file_path,
                    execution_id=execution_id,
                    group_id=group_id,
                    agent_ids=agent_ids,
                    user_token=user_token
                )
            except Exception as embed_error:
                logger.error(f"[UPLOAD] EMBEDDING FAILED: {embed_error}", exc_info=True)
                embedding_result = {"status": "error", "message": f"Embedding failed: {embed_error}"}

            logger.info(f"[UPLOAD] Single embedding completed with result: {embedding_result}")

            # Return unified response regardless of upload method
            response = {
                "status": "success",
                "path": file_path,
                "filename": file.filename,
                "size": file_size,
                "execution_id": execution_id,
                "group_id": group_id,
                "uploaded_at": datetime.now().isoformat(),
                "selected_agents": selected_agents,
                "embedding_result": embedding_result,
                "upload_method": upload_method,
                "volume_info": {
                    "catalog": catalog,
                    "schema": schema,
                    "volume": volume,
                    "full_path": file_path
                },
                "message": f"File {file.filename} uploaded successfully via {upload_method}",
                "simulated": not upload_successful
            }

            logger.info(f"Returning unified response: {response}")
            return response

        except Exception as e:
            logger.error("="*60)
            logger.error(f"ERROR in upload_knowledge_file: {str(e)}")
            logger.error("="*60, exc_info=True)
            # Return error response instead of raising
            return {
                "status": "error",
                "message": str(e),
                "filename": file.filename if file else "unknown"
            }



    async def _read_file_content(self, file_path: str, user_token: Optional[str] = None) -> Optional[str]:
        """Read content from Databricks Volume file."""
        try:
            print(f"ðŸ”¥ DEBUG: _read_file_content called for {file_path}")
            logger.info(f"[FILE READ] About to read file: {file_path}")
            logger.info(f"[FILE READ] Group ID: {self.group_id}")
            logger.info(f"[FILE READ] User token provided: {user_token is not None}")

            result = await self.read_knowledge_file(
                file_path=file_path,
                group_id=self.group_id,
                user_token=user_token
            )

            logger.info(f"[FILE READ] read_knowledge_file returned: {result}")

            if result.get('status') == 'success':
                content = result.get('content', '')
                logger.info(f"[FILE READ] Successfully read {len(content)} characters from {file_path}")
                logger.info(f"[FILE READ] Content preview: {content[:100]}...")
                return content
            else:
                logger.error(f"[FILE READ] Failed to read file {file_path}: {result.get('message')}")
                logger.error(f"[FILE READ] Full result: {result}")
                return None

        except Exception as e:
            logger.error(f"[FILE READ] Exception reading file {file_path}: {e}")
            import traceback
            logger.error(f"[FILE READ] Full traceback: {traceback.format_exc()}")
            return None

    async def _chunk_with_context(
        self,
        content: str,
        file_path: str,
        chunk_size: int = 1000,
        overlap: int = 200
    ) -> List[Dict[str, Any]]:
        """
        Create chunks with document-level context prepended.
        Implements Anthropic's Contextual Retrieval approach.

        Args:
            content: Full document content
            file_path: Path to the file
            chunk_size: Maximum chunk size in characters
            overlap: Overlap between chunks

        Returns:
            List of chunk dictionaries with context
        """
        filename = file_path.split('/')[-1]

        # Generate document summary for context
        document_summary = await self._generate_document_summary(content, filename)
        logger.info(f"[CHUNKING] Generated summary for {filename}: {document_summary[:100]}...")

        # Split content into paragraphs for semantic boundaries
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

        chunks = []
        current_chunk = ""
        chunk_index = 0

        for paragraph in paragraphs:
            # Check if adding this paragraph exceeds chunk size
            if current_chunk and len(current_chunk) + len(paragraph) > chunk_size:
                # Create contextual chunk
                contextual_content = self._create_contextual_chunk(
                    raw_content=current_chunk.strip(),
                    filename=filename,
                    document_summary=document_summary,
                    chunk_index=chunk_index,
                    total_chunks_estimate=len(paragraphs) // 2
                )

                chunks.append({
                    'content': contextual_content,  # For embedding (with context)
                    'raw_content': current_chunk.strip(),  # For display
                    'chunk_index': chunk_index,
                    'section': f"Section {chunk_index + 1}",
                    'document_summary': document_summary
                })

                # Start new chunk with overlap
                overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
                current_chunk = overlap_text + "\n\n" + paragraph
                chunk_index += 1
            else:
                current_chunk = f"{current_chunk}\n\n{paragraph}" if current_chunk else paragraph

        # Add final chunk
        if current_chunk.strip():
            contextual_content = self._create_contextual_chunk(
                raw_content=current_chunk.strip(),
                filename=filename,
                document_summary=document_summary,
                chunk_index=chunk_index,
                total_chunks_estimate=chunk_index + 1
            )

            chunks.append({
                'content': contextual_content,
                'raw_content': current_chunk.strip(),
                'chunk_index': chunk_index,
                'section': f"Section {chunk_index + 1}",
                'document_summary': document_summary
            })

        # Ensure at least one chunk
        if not chunks and content.strip():
            contextual_content = self._create_contextual_chunk(
                raw_content=content.strip(),
                filename=filename,
                document_summary=document_summary,
                chunk_index=0,
                total_chunks_estimate=1
            )
            chunks.append({
                'content': contextual_content,
                'raw_content': content.strip(),
                'chunk_index': 0,
                'section': "Section 1",
                'document_summary': document_summary
            })

        logger.info(f"[CHUNKING] Created {len(chunks)} context-enriched chunks for {filename}")
        return chunks

    def _create_contextual_chunk(
        self,
        raw_content: str,
        filename: str,
        document_summary: str,
        chunk_index: int,
        total_chunks_estimate: int
    ) -> str:
        """
        Create a chunk with prepended document context.

        This context helps the embedding model understand where this chunk
        fits in the larger document, improving retrieval accuracy by 49-67%.
        """
        return f"""Document: {filename}
Summary: {document_summary}
Section: {chunk_index + 1} of ~{total_chunks_estimate}

Content:
{raw_content}"""

    async def _generate_document_summary(self, content: str, filename: str) -> str:
        """
        Generate a brief summary of the document for contextual retrieval.
        Uses a fast model with prompt caching for efficiency.

        Args:
            content: Full document content
            filename: Name of the file

        Returns:
            2-3 sentence summary of the document
        """
        try:
            from src.core.llm_manager import LLMManager

            # Use first 3000 chars for summary generation
            content_preview = content[:3000]

            prompt = f"""Provide a concise 2-3 sentence summary that describes the main topic and purpose of this document.

Document: {filename}
Content preview:
{content_preview}

Summary (2-3 sentences):"""

            # Use fast, efficient model
            llm = LLMManager.get_llm(model_name="databricks-meta-llama-3-3-70b-instruct")
            response = await llm.ainvoke(prompt)

            summary = response.content.strip()
            logger.info(f"[SUMMARY] Generated summary for {filename}")
            return summary

        except Exception as e:
            logger.warning(f"[SUMMARY] Failed to generate summary for {filename}: {e}")
            # Fallback to simple description
            return f"Content from {filename}"

    def _detect_content_type(self, filename: str) -> str:
        """
        Detect content type from file extension.

        Args:
            filename: Name of the file

        Returns:
            Content type string
        """
        ext = filename.lower().split('.')[-1] if '.' in filename else ''

        type_map = {
            'pdf': 'application/pdf',
            'txt': 'text/plain',
            'md': 'text/markdown',
            'json': 'application/json',
            'csv': 'text/csv',
            'xml': 'application/xml',
            'html': 'text/html',
            'doc': 'application/msword',
            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        }

        return type_map.get(ext, 'application/octet-stream')

    async def read_knowledge_file(
        self,
        file_path: str,
        group_id: str,
        user_token: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Read content from a file in Databricks Volume.
        
        Args:
            file_path: Full path to the file in Databricks Volume
            group_id: Group ID for tenant isolation
            user_token: Optional user token for OBO authentication
            
        Returns:
            File content and metadata
        """
        import aiohttp
        
        logger.info("="*60)
        logger.info("STARTING KNOWLEDGE FILE READ")
        logger.info(f"File Path: {file_path}")
        logger.info(f"Group ID: {group_id}")
        logger.info("="*60)
        
        try:
            # Get Databricks configuration
            config = await self.repository.get_active_config(group_id=group_id)
            if not config:
                # Get from unified auth
                workspace_url = 'https://example.databricks.com'
                token = ''
                try:
                    from src.utils.databricks_auth import get_auth_context
                    auth = await get_auth_context()
                    if auth:
                        workspace_url = auth.workspace_url
                        token = auth.token
                except Exception as e:
                    logger.warning(f"Failed to get unified auth: {e}")

                config = type('obj', (object,), {
                    'workspace_url': workspace_url,
                    'encrypted_personal_access_token': token
                })()

            # Get workspace URL from config or unified auth
            workspace_url = getattr(config, 'workspace_url', None)
            if not workspace_url:
                from src.utils.databricks_auth import get_auth_context
                auth = await get_auth_context()
                if auth:
                    workspace_url = auth.workspace_url
            logger.info(f"Workspace URL: {workspace_url}")

            # Get authentication token using unified auth
            from src.utils.databricks_auth import get_auth_context

            try:
                auth = await get_auth_context(user_token=user_token)
                if not auth:
                    raise Exception("Failed to get authentication context")
                token = auth.token
                workspace_url = auth.workspace_url
                logger.info(f"Successfully obtained Databricks authentication: {auth.auth_method}")
            except Exception as auth_error:
                logger.warning(f"Auth failed: {auth_error}")
                token = None
            
            if token and workspace_url and workspace_url != 'https://example.databricks.com':
                logger.info("Attempting REAL file read from Databricks")
                
                # Try using Databricks SDK via centralized auth
                workspace_client = await get_workspace_client(user_token=token)
                if workspace_client:
                    try:
                        logger.info("Using WorkspaceClient from databricks_auth middleware to read file")

                        # Read file using SDK
                        download_response = workspace_client.files.download(file_path)
                        content = download_response.contents.read()
                            
                        logger.info(f"Successfully read file: {len(content)} bytes")
                        
                        # Check if it's a PDF file
                        filename = os.path.basename(file_path).lower()
                        if filename.endswith('.pdf'):
                            logger.info("Detected PDF file, extracting text content")
                            try:
                                # Try pypdf (newer) first, then fall back to PyPDF2
                                try:
                                    from pypdf import PdfReader
                                except ImportError:
                                    from PyPDF2 import PdfReader
                                
                                import io
                                
                                pdf_reader = PdfReader(io.BytesIO(content))
                                text_content = ""
                                for page_num in range(len(pdf_reader.pages)):
                                    page = pdf_reader.pages[page_num]
                                    text_content += page.extract_text() + "\n"
                                
                                logger.info(f"Extracted {len(text_content)} chars from PDF")
                                content = text_content
                            except ImportError:
                                logger.warning("PDF library not installed, returning placeholder")
                                # Fallback: just decode what we can
                                content = f"[PDF File: {filename}]\n[Unable to extract text - PDF library not installed]"
                            except Exception as pdf_error:
                                logger.error(f"Error extracting PDF text: {pdf_error}")
                                content = f"[PDF File: {filename}]\n[Error extracting text: {pdf_error}]"
                        else:
                            # For non-PDF files, decode as text
                            content = content.decode('utf-8') if isinstance(content, bytes) else content
                        
                        return {
                            "status": "success",
                            "path": file_path,
                            "content": content,
                            "size": len(content),
                            "filename": os.path.basename(file_path)
                        }
                    except Exception as sdk_error:
                        logger.error(f"SDK read failed: {sdk_error}")
                        # Fall through to API method
                
                # Fallback: Use Files API
                logger.info("Using Files API to read file")
                async with aiohttp.ClientSession() as session:
                    headers = {
                        "Authorization": f"Bearer {token}"
                    }
                    
                    # Use Files API to read the file
                    read_url = f"{workspace_url}/api/2.0/fs/files{file_path}"
                    logger.info(f"Reading from: {read_url}")
                    
                    async with session.get(read_url, headers=headers) as response:
                        if response.status == 200:
                            # Read as binary first
                            content_bytes = await response.read()
                            logger.info(f"Successfully read file: {len(content_bytes)} bytes")
                            
                            # Check if it's a PDF file
                            filename = os.path.basename(file_path).lower()
                            if filename.endswith('.pdf'):
                                logger.info("Detected PDF file, extracting text content")
                                try:
                                    # Try pypdf (newer) first, then fall back to PyPDF2
                                    try:
                                        from pypdf import PdfReader
                                    except ImportError:
                                        from PyPDF2 import PdfReader
                                    
                                    import io
                                    
                                    pdf_reader = PdfReader(io.BytesIO(content_bytes))
                                    text_content = ""
                                    for page_num in range(len(pdf_reader.pages)):
                                        page = pdf_reader.pages[page_num]
                                        text_content += page.extract_text() + "\n"
                                    
                                    logger.info(f"Extracted {len(text_content)} chars from PDF")
                                    content = text_content
                                except ImportError:
                                    logger.warning("PDF library not installed, returning placeholder")
                                    content = f"[PDF File: {filename}]\n[Unable to extract text - PDF library not installed]"
                                except Exception as pdf_error:
                                    logger.error(f"Error extracting PDF text: {pdf_error}")
                                    content = f"[PDF File: {filename}]\n[Error extracting text: {pdf_error}]"
                            else:
                                # For non-PDF files, decode as text
                                try:
                                    content = content_bytes.decode('utf-8')
                                except UnicodeDecodeError:
                                    logger.warning("Unable to decode as UTF-8, trying latin-1")
                                    content = content_bytes.decode('latin-1', errors='ignore')
                            
                            return {
                                "status": "success",
                                "path": file_path,
                                "content": content,
                                "size": len(content),
                                "filename": os.path.basename(file_path)
                            }
                        else:
                            error_text = await response.text()
                            logger.error(f"Failed to read file: Status {response.status}")
                            logger.error(f"Error: {error_text}")
                            raise Exception(f"Failed to read file: {error_text}")
            else:
                logger.warning("Missing credentials - returning mock content")
                
                # Return mock content for testing
                return {
                    "status": "success",
                    "path": file_path,
                    "content": f"Mock content for file: {file_path}\nThis would contain the actual file content from Databricks Volume.",
                    "size": 100,
                    "filename": os.path.basename(file_path),
                    "mock": True
                }
                
        except Exception as e:
            logger.error(f"Error reading knowledge file: {e}", exc_info=True)
            return {
                "status": "error",
                "message": str(e),
                "path": file_path
            }
    
    async def browse_volume_files(
        self,
        volume_path: str,
        group_id: str,
        user_token: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Browse files in a Databricks Volume directory.
        
        Args:
            volume_path: Path to browse (format: catalog.schema.volume/optional/path)
            group_id: Group ID for tenant isolation
            user_token: Optional user token for OBO authentication
            
        Returns:
            List of files and directories with metadata
        """
        import aiohttp
        
        logger.info("=" * 60)
        logger.info("STARTING VOLUME FILES BROWSE")
        logger.info(f"Volume Path: {volume_path}")
        logger.info(f"Group ID: {group_id}")
        logger.info("=" * 60)
        
        try:
            # Get Databricks configuration
            config = await self.repository.get_active_config(group_id=group_id)
            if not config:
                # Get from unified auth
                workspace_url = 'https://example.databricks.com'
                token = ''
                try:
                    from src.utils.databricks_auth import get_auth_context
                    auth = await get_auth_context()
                    if auth:
                        workspace_url = auth.workspace_url
                        token = auth.token
                except Exception as e:
                    logger.warning(f"Failed to get unified auth: {e}")

                config = type('obj', (object,), {
                    'knowledge_volume_enabled': True,
                    'knowledge_volume_path': 'main.default.knowledge',
                    'workspace_url': workspace_url,
                    'encrypted_personal_access_token': token
                })()
                logger.info("Using default configuration")
            else:
                logger.info(f"Found Databricks config - workspace_url: {config.workspace_url}")
                logger.info(f"Config knowledge_volume_path: {getattr(config, 'knowledge_volume_path', 'NOT SET')}")
            
            # Parse the volume path to construct the full directory path
            if volume_path.startswith('/Volumes/'):
                # Already a full path
                directory_path = volume_path
                logger.info(f"Using provided full path: {directory_path}")
            else:
                # Construct full path from volume_path (e.g., "catalog.schema.volume/path")
                if '/' in volume_path:
                    volume_part, sub_path = volume_path.split('/', 1)
                    logger.info(f"Split path: volume_part='{volume_part}', sub_path='{sub_path}'")
                else:
                    volume_part = volume_path
                    sub_path = ""
                    logger.info(f"No sub-path: volume_part='{volume_part}'")
                
                # Parse volume part (catalog.schema.volume)
                parts = volume_part.split('.')
                logger.info(f"Volume parts: {parts} (count: {len(parts)})")
                
                if len(parts) == 3:
                    catalog, schema, volume = parts
                    directory_path = f"/Volumes/{catalog}/{schema}/{volume}"
                    if sub_path:
                        directory_path = f"{directory_path}/{sub_path}"
                    logger.info(f"Constructed path from parts: catalog={catalog}, schema={schema}, volume={volume}")
                    logger.info(f"Final constructed path: {directory_path}")
                else:
                    # Use configured volume path as fallback
                    configured_volume = getattr(config, 'knowledge_volume_path', 'main.default.knowledge')
                    parts_fallback = configured_volume.split('.')
                    if len(parts_fallback) == 3:
                        catalog, schema, volume = parts_fallback
                        directory_path = f"/Volumes/{catalog}/{schema}/{volume}"
                        logger.info(f"Used fallback config path: {configured_volume}")
                        logger.info(f"Fallback constructed path: {directory_path}")
                    else:
                        logger.error(f"Invalid volume path format: {configured_volume}")
                        return []
            
            # Get authentication token using unified auth
            from src.utils.databricks_auth import get_auth_context

            # Get workspace URL from config or unified auth
            workspace_url = getattr(config, 'workspace_url', None)
            if not workspace_url:
                auth = await get_auth_context()
                if auth:
                    workspace_url = auth.workspace_url
            logger.info(f"Workspace URL: {workspace_url}")
            logger.info(f"FINAL DIRECTORY PATH TO BROWSE: {directory_path}")

            try:
                auth = await get_auth_context(user_token=user_token)
                if not auth:
                    raise Exception("Failed to get authentication context")
                token = auth.token
                workspace_url = auth.workspace_url
                logger.info(f"Successfully obtained Databricks authentication: {auth.auth_method} (token length: {len(token)})")
            except Exception as auth_error:
                logger.error(f"Authentication failed: {auth_error}")
                return []
            
            if not token:
                logger.error("No authentication token available")
                return []
                
            if not workspace_url or workspace_url == 'https://example.databricks.com':
                logger.error(f"Invalid workspace URL: {workspace_url}")
                return []
            
            logger.info("Starting REAL Databricks API calls")
            
            # Try using Databricks SDK via centralized auth
            workspace_client = await get_workspace_client(user_token=token)
            if workspace_client:
                try:
                    logger.info("Using WorkspaceClient from databricks_auth middleware for directory listing")

                    logger.info(f"SDK: Calling list_directory_contents('{directory_path}')")

                    # List directory contents using SDK
                    files_list = []
                    file_infos = workspace_client.files.list_directory_contents(directory_path)
                    
                    for file_info in file_infos:
                        file_data = {
                            "name": os.path.basename(file_info.path),
                            "path": file_info.path,
                            "is_directory": file_info.is_directory,
                            "size": file_info.file_size if hasattr(file_info, 'file_size') and file_info.file_size else None,
                            "modified_at": file_info.modification_time.isoformat() if hasattr(file_info, 'modification_time') and file_info.modification_time else None,
                            "type": "directory" if file_info.is_directory else self._get_file_type(os.path.basename(file_info.path))
                        }
                        files_list.append(file_data)
                        logger.info(f"SDK: Found item: {file_data['name']} ({'dir' if file_data['is_directory'] else 'file'})")
                    
                    logger.info(f"SDK: Successfully found {len(files_list)} items")
                    return files_list
                    
                except Exception as sdk_error:
                    logger.error(f"SDK method failed: {sdk_error}")
                    logger.info("Falling back to REST API method")
            
            # Fallback: Use Files REST API
            logger.info("Attempting Files REST API method")
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {token}",
                    "Content-Type": "application/json"
                }
                
                # Use Files API to list directory - correct Unity Catalog format
                list_url = f"{workspace_url}/api/2.0/fs/directories{directory_path}"
                logger.info(f"API: Request URL: {list_url}")
                logger.info(f"API: Headers: {headers}")
                
                async with session.get(list_url, headers=headers) as response:
                    logger.info(f"API: Response status: {response.status}")
                    
                    if response.status == 200:
                        result = await response.json()
                        logger.info(f"API: Response data keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
                        
                        files_list = []
                        contents = result.get('contents', [])
                        logger.info(f"API: Found {len(contents)} contents items")
                        
                        for item in contents:
                            file_data = {
                                "name": os.path.basename(item['path']),
                                "path": item['path'],
                                "is_directory": item.get('is_directory', False),
                                "size": item.get('file_size'),
                                "modified_at": item.get('modification_time'),
                                "type": "directory" if item.get('is_directory', False) else self._get_file_type(os.path.basename(item['path']))
                            }
                            files_list.append(file_data)
                            logger.info(f"API: Found item: {file_data['name']} ({'dir' if file_data['is_directory'] else 'file'})")
                        
                        logger.info(f"API: Successfully returned {len(files_list)} items")
                        return files_list
                        
                    elif response.status == 404:
                        logger.warning(f"Directory not found (404): {directory_path}")
                        return []
                    elif response.status == 403:
                        logger.error(f"Access forbidden (403): {directory_path}")
                        return []
                    elif response.status == 401:
                        logger.error(f"Unauthorized (401): Check token permissions")
                        return []
                    else:
                        error_text = await response.text()
                        logger.error(f"API error {response.status}: {error_text}")
                        return []
                        
        except Exception as e:
            logger.error(f"Exception in browse_volume_files: {e}", exc_info=True)
            return []
    
    async def register_volume_file(
        self,
        execution_id: str,
        file_path: str,
        group_id: str
    ) -> Dict[str, Any]:
        """
        Register an existing Databricks Volume file for use as knowledge source.
        
        Args:
            execution_id: Execution ID for scoping
            file_path: Full path to the file in Databricks Volume
            group_id: Group ID for tenant isolation
            
        Returns:
            Registration confirmation with file metadata
        """
        try:
            # Get file name from path
            filename = os.path.basename(file_path)
            
            # For now, simulate successful registration
            logger.info(f"Simulating registration of file {filename} from volume")
            
            # Return registration metadata
            return {
                "status": "success",
                "path": file_path,
                "filename": filename,
                "execution_id": execution_id,
                "group_id": group_id,
                "registered_at": datetime.now().isoformat(),
                "source": "volume",
                "message": f"File {filename} registered successfully from Databricks Volume"
            }
                
        except Exception as e:
            logger.error(f"Error registering volume file: {e}")
            raise
    
    def _get_file_type(self, filename: str) -> str:
        """
        Determine file type from extension.
        
        Args:
            filename: Name of the file
            
        Returns:
            File type string
        """
        ext = os.path.splitext(filename)[1].lower()
        type_map = {
            '.pdf': 'pdf',
            '.txt': 'text',
            '.md': 'markdown',
            '.json': 'json',
            '.csv': 'csv',
            '.doc': 'word',
            '.docx': 'word',
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.yaml': 'yaml',
            '.yml': 'yaml',
            '.xml': 'xml',
            '.html': 'html'
        }
        return type_map.get(ext, 'file')
    
    async def list_knowledge_files(
        self,
        execution_id: str,
        group_id: str
    ) -> List[Dict[str, Any]]:
        """
        List all knowledge files for a specific execution.
        
        Args:
            execution_id: Execution ID to list files for
            group_id: Group ID for tenant isolation
            
        Returns:
            List of files with metadata
        """
        try:
            # For now, return an empty list since we're simulating uploads
            logger.info(f"Listing knowledge files for execution {execution_id}, group {group_id}")
            
            # In a real implementation, this would list files from Databricks Volume
            # For now, return empty list to avoid hanging
            return []
            
        except Exception as e:
            logger.error(f"Error listing knowledge files: {e}")
            return []
    
    async def delete_knowledge_file(
        self,
        execution_id: str,
        group_id: str,
        filename: str
    ) -> bool:
        """
        Delete a knowledge file from Databricks Volume.
        
        Args:
            execution_id: Execution ID of the file
            group_id: Group ID for tenant isolation
            filename: Name of the file to delete
            
        Returns:
            True if deletion was successful
        """
        try:
            # For now, simulate successful deletion
            logger.info(f"Simulating deletion of file {filename} for execution {execution_id}, group {group_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error deleting knowledge file: {e}")
            return False
    
    async def _ensure_volume_exists(self, workspace_client, config) -> None:
        """
        Ensure the Databricks Volume exists, create if it doesn't.
        
        Args:
            workspace_client: Databricks workspace client
            config: Databricks configuration
        """
        try:
            volume_path = config.knowledge_volume_path
            if not volume_path:
                volume_path = "main.default.knowledge"
            
            parts = volume_path.split('.')
            if len(parts) != 3:
                raise ValueError(f"Invalid volume path format: {volume_path}")
            
            catalog, schema, volume = parts
            
            # Try to create the volume (will fail silently if it exists)
            try:
                # Create catalog if needed
                workspace_client.catalogs.create(name=catalog, comment="Created by Kasal")
            except Exception:
                # Catalog might already exist
                pass
            
            try:
                # Create schema if needed
                workspace_client.schemas.create(
                    name=schema,
                    catalog_name=catalog,
                    comment="Created by Kasal for knowledge storage"
                )
            except Exception:
                # Schema might already exist
                pass
            
            try:
                # Create volume if needed
                workspace_client.volumes.create(
                    name=volume,
                    catalog_name=catalog,
                    schema_name=schema,
                    volume_type="MANAGED",
                    comment="Knowledge source storage for Kasal"
                )
                logger.info(f"Created volume: {volume_path}")
            except Exception as e:
                # Volume might already exist or other error
                logger.debug(f"Volume creation info: {e}")
                
        except Exception as e:
            logger.error(f"Error ensuring volume exists: {e}")
            # Continue anyway, the actual file operations will fail if there's a real problem
    
    def _get_workspace_client(self, config):
        """
        Get Databricks workspace client.

        Args:
            config: Databricks configuration

        Returns:
            Configured WorkspaceClient or None if SDK not available
        """
        # For now, always return None to avoid SDK issues
        logger.warning("Databricks SDK support is currently disabled for testing")
        return None

    async def _embed_uploaded_file(
        self,
        file_path: str,
        execution_id: str,
        group_id: str,
        agent_ids: Optional[List[str]] = None,
        user_token: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Embed an uploaded file immediately into Vector Search.

        Args:
            file_path: Full path to the file in Databricks Volume
            execution_id: Execution ID for scoping
            group_id: Group ID for multi-tenant filtering
            agent_ids: Optional list of agent IDs that can access this knowledge source
            user_token: Optional user token for OBO authentication

        Returns:
            Embedding result with status and metadata
        """
        try:
            print(f"ðŸ”¥ DEBUG: _embed_uploaded_file called for {file_path}")
            logger.info(f"[EMBEDDING] Starting immediate embedding for file: {file_path}")

            # CRITICAL DEBUG: Log the agent_ids parameter we received
            logger.info(f"[EMBEDDING] ðŸ” AGENT_IDS RECEIVED IN EMBED FUNCTION: {agent_ids}")
            logger.info(f"[EMBEDDING] ðŸ” Agent IDs type: {type(agent_ids)}, length: {len(agent_ids) if agent_ids else 0}")
            if agent_ids:
                logger.info(f"[EMBEDDING] âœ… Agent IDs detected in embedding: {agent_ids}")
            else:
                logger.warning(f"[EMBEDDING] âš ï¸ NO AGENT_IDS in embedding function - this is the problem!")

            # 1. Read file content (simulate if file doesn't exist locally)
            print(f"ðŸ”¥ DEBUG: About to call _read_file_content for {file_path}")
            content = await self._read_file_content(file_path, user_token)
            print(f"ðŸ”¥ DEBUG: _read_file_content returned {len(content or '')} characters")
            logger.info(f"ðŸ”¥ CRITICAL DEBUG: Content preview from file: {content[:200] if content else 'NO CONTENT'}...")

            if not content:
                logger.warning(f"[EMBEDDING] No content found for file: {file_path}")
                return {"status": "skipped", "reason": "No content to embed"}

            logger.info(f"[EMBEDDING] Read {len(content)} characters from file")

            # 2. Chunk the content with context enrichment
            chunks = await self._chunk_with_context(content, file_path)
            if not chunks:
                logger.warning(f"[EMBEDDING] No chunks generated for file: {file_path}")
                return {"status": "skipped", "reason": "No chunks generated"}

            logger.info(f"[EMBEDDING] Generated {len(chunks)} context-enriched chunks")

            # 3. Get Vector Search storage (same pattern as DocumentationEmbeddingService)
            vector_storage = await self._get_vector_storage(user_token)
            if not vector_storage:
                logger.warning(f"[EMBEDDING] Vector search not configured - skipping embedding")
                return {"status": "skipped", "reason": "Vector search not configured"}

            logger.info(f"[EMBEDDING] Using vector index: {vector_storage.index_name}")

            # 4. Process each chunk
            embedded_chunks = 0
            filename = file_path.split('/')[-1]

            for i, chunk_data in enumerate(chunks):
                try:
                    # Extract chunk information
                    chunk_content = chunk_data['content']  # Contextual content (for embedding)
                    raw_content = chunk_data.get('raw_content', chunk_content)  # Raw content (for display)
                    section = chunk_data.get('section', f'Section {i+1}')
                    document_summary = chunk_data.get('document_summary', '')

                    # Prepare enhanced metadata for this chunk
                    metadata = {
                        'source': file_path,
                        'filename': filename,
                        'execution_id': execution_id,
                        'group_id': group_id,
                        'chunk_index': i,
                        'total_chunks': len(chunks),
                        'section': section,  # NEW: Section information
                        'parent_document_id': f"{group_id}:{execution_id}:{filename}",  # NEW: Parent document ID
                        'document_summary': document_summary,  # NEW: Document summary for context
                        'file_path': file_path,
                        'created_at': datetime.utcnow().isoformat(),
                        'type': 'knowledge_source',  # IMPORTANT: Sets document_type to "knowledge_source"
                        'content_type': self._detect_content_type(filename)  # NEW: Detected content type
                    }

                    # Generate embedding for the chunk (vector storage will handle this)
                    # Convert agent_ids to JSON string for storage (same as crew knowledge source)
                    import json
                    agent_ids_json = json.dumps(agent_ids) if agent_ids else json.dumps([])
                    logger.info(f"[EMBEDDING] Chunk {i}: agent_ids={agent_ids}, json={agent_ids_json}")

                    # CRITICAL DEBUG: Show what we're passing to vector storage
                    logger.info(f"[EMBEDDING] ðŸ” CHUNK {i} DATA BEING SENT TO VECTOR STORAGE:")
                    logger.info(f"[EMBEDDING]   - agent_ids: {agent_ids}")
                    logger.info(f"[EMBEDDING]   - agent_ids_json: {agent_ids_json}")
                    logger.info(f"[EMBEDDING]   - group_id: {group_id}")
                    logger.info(f"[EMBEDDING]   - section: {section}")

                    data = {
                        'content': chunk_content,  # Contextual content with document summary
                        'agent_ids': agent_ids_json,  # JSON array of agent IDs for access control
                        'group_id': group_id,  # Top-level field for document schema
                        'metadata': metadata,
                        'context': {
                            'query_text': f"Knowledge file: {file_path.split('/')[-1]}",
                            'session_id': execution_id,
                            'interaction_sequence': i
                        }
                    }

                    # Save to vector storage
                    await vector_storage.save(data)
                    embedded_chunks += 1

                    logger.info(f"[EMBEDDING] Embedded chunk {i+1}/{len(chunks)} for file: {file_path}")

                except Exception as chunk_error:
                    logger.error(f"[EMBEDDING] Error embedding chunk {i}: {chunk_error}")
                    continue

            logger.info(f"[EMBEDDING] Successfully embedded {embedded_chunks}/{len(chunks)} chunks for file: {file_path}")

            return {
                "status": "success",
                "chunks_processed": len(chunks),
                "chunks_embedded": embedded_chunks,
                "index_name": vector_storage.index_name,
                "message": f"Successfully embedded {embedded_chunks} chunks from {file_path}"
            }

        except Exception as e:
            logger.error(f"[EMBEDDING] Error embedding uploaded file {file_path}: {e}", exc_info=True)
            return {
                "status": "error",
                "error": str(e),
                "message": f"Failed to embed file {file_path}"
            }


    async def _chunk_text(self, content: str, file_path: str) -> List[str]:
        """
        Split text content into chunks for embedding.

        Args:
            content: Text content to chunk
            file_path: File path for logging

        Returns:
            List of text chunks
        """
        try:
            # Simple chunking strategy: split by paragraphs and combine to ~1000 chars
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

            chunks = []
            current_chunk = ""

            for paragraph in paragraphs:
                # If adding this paragraph would exceed 1000 chars, save current chunk
                if current_chunk and len(current_chunk) + len(paragraph) > 1000:
                    chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                else:
                    if current_chunk:
                        current_chunk += "\n\n" + paragraph
                    else:
                        current_chunk = paragraph

            # Add the last chunk
            if current_chunk.strip():
                chunks.append(current_chunk.strip())

            # Ensure we have at least one chunk
            if not chunks and content.strip():
                chunks = [content.strip()]

            logger.info(f"[CHUNKING] Split {len(content)} chars into {len(chunks)} chunks for {file_path}")
            return chunks

        except Exception as e:
            logger.error(f"[CHUNKING] Error chunking text for {file_path}: {e}")
            return []

    async def _get_vector_storage(self, user_token: Optional[str] = None):
        """
        Get Vector Search storage instance using same pattern as DocumentationEmbeddingService.

        Args:
            user_token: Optional user token for OBO authentication

        Returns:
            DatabricksVectorStorage instance or None if not configured
        """
        try:
            from src.core.unit_of_work import UnitOfWork
            from src.engines.crewai.memory.databricks_vector_storage import DatabricksVectorStorage
            from src.schemas.memory_backend import MemoryBackendConfig, MemoryBackendType

            logger.info(f"[VECTOR_STORAGE] Looking for active Databricks configuration for group: {self.group_id}")

            # Use Unit of Work pattern to get memory backends FOR THIS GROUP ONLY
            async with UnitOfWork() as uow:
                group_backends = await uow.memory_backend_repository.get_by_group_id(self.group_id)

            # Filter active Databricks backends for this group
            databricks_backends = [
                b for b in group_backends
                if b.is_active and b.backend_type == MemoryBackendType.DATABRICKS
            ]

            if not databricks_backends:
                logger.warning("[VECTOR_STORAGE] No active Databricks memory backend found for this group; skipping vector search usage")
                return None

            # Sort by created_at descending and take the most recent
            databricks_backends.sort(key=lambda x: x.created_at, reverse=True)
            backend = databricks_backends[0]
            logger.info(f"[VECTOR_STORAGE] Using Databricks backend from group: {backend.group_id}, created: {backend.created_at}")

            # Convert backend model to config schema (same pattern as DocumentationEmbeddingService)
            memory_config = MemoryBackendConfig(
                backend_type=backend.backend_type,
                databricks_config=backend.databricks_config,
                enable_short_term=backend.enable_short_term,
                enable_long_term=backend.enable_long_term,
                enable_entity=backend.enable_entity,
                custom_config=backend.custom_config
            )

            # Get databricks config
            db_config = memory_config.databricks_config
            if not db_config:
                logger.warning("[VECTOR_STORAGE] Databricks configuration is None, cannot initialize storage")
                return None

            # Check for document index (same logic as DocumentationEmbeddingService)
            document_index = None
            if hasattr(db_config, 'document_index'):
                document_index = db_config.document_index
            elif isinstance(db_config, dict):
                document_index = db_config.get('document_index')

            if not document_index:
                logger.warning("[VECTOR_STORAGE] Document index not configured in Databricks backend")
                logger.info(f"[VECTOR_STORAGE] Available config keys: {list(db_config.keys()) if isinstance(db_config, dict) else 'Not a dict'}")
                return None

            logger.info(f"[VECTOR_STORAGE] Found document index: {document_index}")

            # Extract configuration values - handle both dict and object forms (same as DocumentationEmbeddingService)
            if hasattr(db_config, 'endpoint_name'):
                # It's an object
                endpoint_name = getattr(db_config, 'document_endpoint_name', None) or db_config.endpoint_name
                workspace_url = db_config.workspace_url
                embedding_dimension = db_config.embedding_dimension or 1024
                personal_access_token = db_config.personal_access_token
                service_principal_client_id = db_config.service_principal_client_id
                service_principal_client_secret = db_config.service_principal_client_secret
            elif isinstance(db_config, dict):
                # It's a dictionary
                endpoint_name = db_config.get('document_endpoint_name') or db_config.get('endpoint_name')
                workspace_url = db_config.get('workspace_url')
                embedding_dimension = db_config.get('embedding_dimension', 1024)
                personal_access_token = db_config.get('personal_access_token')
                service_principal_client_id = db_config.get('service_principal_client_id')
                service_principal_client_secret = db_config.get('service_principal_client_secret')
            else:
                logger.error(f"[VECTOR_STORAGE] Unexpected databricks_config type: {type(db_config)}")
                return None

            logger.info(f"[VECTOR_STORAGE] Configuration extracted - endpoint: {endpoint_name}, workspace: {workspace_url}")

            # Create storage instance (same constructor as DocumentationEmbeddingService)
            storage = DatabricksVectorStorage(
                endpoint_name=endpoint_name,
                index_name=document_index,
                crew_id="knowledge_files",  # Dedicated crew ID for knowledge files
                memory_type="document",
                embedding_dimension=embedding_dimension,
                workspace_url=workspace_url,
                personal_access_token=personal_access_token,
                service_principal_client_id=service_principal_client_id,
                service_principal_client_secret=service_principal_client_secret,
                user_token=user_token
            )

            logger.info(f"[VECTOR_STORAGE] Created storage for index: {storage.index_name}")
            return storage

        except Exception as e:
            logger.error(f"[VECTOR_STORAGE] Error creating vector storage: {e}", exc_info=True)
            return None

    async def search_knowledge(
        self,
        query: str,
        group_id: str,
        execution_id: Optional[str] = None,
        file_paths: Optional[List[str]] = None,
        agent_id: Optional[str] = None,
        limit: int = 5,
        user_token: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        VERSION: 2025-10-03-AGENT-FILTER
        Search for knowledge in the Databricks Vector Index.

        This is an engine-agnostic method that can be used by any AI engine.

        Args:
            query: The search query
            group_id: Group ID for tenant isolation
            execution_id: Optional execution ID for scoping
            file_paths: Optional list of file paths to filter search
            agent_id: Optional agent ID for access control filtering
            limit: Maximum number of results to return
            user_token: Optional user token for OBO authentication

        Returns:
            List of search results with content and metadata
        """
        logger.info("="*60)
        logger.info("ðŸ”¥ðŸ”¥ðŸ”¥ [ENTRY POINT] search_knowledge() METHOD CALLED ðŸ”¥ðŸ”¥ðŸ”¥")
        logger.info("="*60)

        try:
            import inspect
            # Check which version of the method is executing
            source = inspect.getsource(self.search_knowledge)
            if 'VERSION:' in source:
                version = source.split('VERSION:')[1].split('\n')[0].strip()
                logger.info(f"ðŸ”¥ðŸ”¥ðŸ”¥ METHOD VERSION CHECK: {version}")
            else:
                logger.info("ðŸ”¥ðŸ”¥ðŸ”¥ METHOD VERSION CHECK: OLD VERSION (no version tag)")
        except Exception as version_error:
            logger.error(f"ðŸ”¥ðŸ”¥ðŸ”¥ VERSION CHECK FAILED: {version_error}", exc_info=True)

        logger.info("ðŸ”¥ðŸ”¥ðŸ”¥ [NEW CODE] REFACTORED SEARCH_KNOWLEDGE METHOD RUNNING ðŸ”¥ðŸ”¥ðŸ”¥")
        logger.info("[SEARCH DEBUG] KNOWLEDGE SEARCH STARTED")
        logger.info(f"[SEARCH DEBUG] Query: '{query}'")
        logger.info(f"[SEARCH DEBUG] Group ID: '{group_id}'")
        logger.info(f"[SEARCH DEBUG] Execution ID: '{execution_id}'")
        logger.info(f"[SEARCH DEBUG] File paths filter: {file_paths}")
        logger.info(f"[SEARCH DEBUG] Limit: {limit}")
        logger.info(f"[SEARCH DEBUG] User token provided: {bool(user_token)}")
        logger.info("="*60)

        try:
            # CRITICAL: Use _get_vector_storage() to ensure upload and search use the same configuration
            # This matches the pattern used by DatabricksVectorStorage for long_term, short_term, entity memory
            logger.info("[SEARCH DEBUG] Getting vector storage configuration (same as upload)...")

            vector_storage = await self._get_vector_storage(user_token)
            if not vector_storage:
                logger.warning("[SEARCH DEBUG] Vector storage not configured, returning empty results")
                return []

            document_index = vector_storage.index_name
            endpoint_name = vector_storage.endpoint_name
            workspace_url = vector_storage.workspace_url

            logger.info(f"[SEARCH DEBUG] Vector storage configuration:")
            logger.info(f"  - document_index: {document_index}")
            logger.info(f"  - endpoint_name: {endpoint_name}")
            logger.info(f"  - workspace_url: {workspace_url}")

            # Import here to avoid circular dependencies
            from src.schemas.databricks_index_schemas import DatabricksIndexSchemas
            from src.core.llm_manager import LLMManager

            # Get the schema for document memory type
            logger.info("[SEARCH DEBUG] Getting schema for document memory type...")
            schema_fields = DatabricksIndexSchemas.get_schema("document")
            search_columns = DatabricksIndexSchemas.get_search_columns("document")
            logger.info(f"[SEARCH DEBUG] Schema fields: {list(schema_fields.keys())}")
            logger.info(f"[SEARCH DEBUG] Search columns: {search_columns}")

            # Generate query embedding
            logger.info(f"[SEARCH DEBUG] Generating embedding for query: '{query}'")
            try:
                query_embedding = await LLMManager.get_embedding(query, model="databricks-gte-large-en")
                if not query_embedding:
                    logger.error("[SEARCH DEBUG] Failed to generate query embedding!")
                    return []
                logger.info(f"[SEARCH DEBUG] Generated embedding with dimension: {len(query_embedding)}")
            except Exception as embed_error:
                logger.error(f"[SEARCH DEBUG] Error generating embedding: {embed_error}", exc_info=True)
                return []

            # Use the repository from vector_storage (same pattern as DatabricksVectorStorage.search())
            logger.info("[SEARCH DEBUG] Using repository from vector_storage...")
            index_repo = vector_storage.repository
            logger.info("[SEARCH DEBUG] Repository retrieved successfully")

            # Quick readiness gate to avoid blocking when endpoint/index is provisioning
            try:
                index_info = await asyncio.wait_for(
                    index_repo.get_index(index_name=document_index, endpoint_name=endpoint_name, user_token=user_token),
                    timeout=3
                )
                ready = False
                try:
                    if hasattr(index_info, 'success'):
                        ready = bool(getattr(index_info, 'success', False) and getattr(getattr(index_info, 'index', None), 'ready', False))
                    elif isinstance(index_info, dict):
                        status = index_info.get('status') or {}
                        ready = bool(status.get('ready') or index_info.get('ready'))
                except Exception:
                    ready = False
                if not ready:
                    logger.info("[SEARCH DEBUG] Databricks index not ready (provisioning). Skipping knowledge search.")
                    return []
            except Exception as e:
                logger.info(f"[SEARCH DEBUG] Skipping knowledge search due to provisioning/unavailable: {e}")
                return []

            # Build filters based on group_id and optional parameters
            logger.info("[SEARCH DEBUG] Building search filters...")
            filters = {
                "group_id": group_id
            }
            logger.info(f"[SEARCH DEBUG] Base filter - group_id: '{group_id}'")

            # IMPORTANT: Search across ALL documents for the group by default
            # This allows knowledge to persist and be reused across executions
            logger.info("[SEARCH DEBUG] âœ… Searching across ALL knowledge documents for group (not execution-scoped)")
            logger.info("[SEARCH DEBUG] This enables knowledge reuse across multiple workflow runs")

            # NOTE: execution_id filtering is intentionally NOT applied by default
            # If you need execution-scoped search, modify the filter manually
            # Uncomment below if execution-specific filtering is needed:
            # if execution_id:
            #     filters["execution_id"] = execution_id
            #     logger.info(f"[SEARCH DEBUG] Applied execution_id filter: '{execution_id}'")

            if file_paths:
                logger.info(f"[SEARCH DEBUG] Adding file_paths filter: {file_paths}")
                filters["source"] = {"$in": file_paths}
            else:
                logger.info("[SEARCH DEBUG] No file_paths filter - will search all documents")

            # CRITICAL: Filter by agent_ids for access control
            # The agent_ids column in the vector index is a JSON array like ["agent-uuid-1", "agent-uuid-2"]
            # We need to check if the agent_id is in that array
            if agent_id:
                logger.info(f"[SEARCH DEBUG] Adding agent_ids filter for agent: {agent_id}")
                # Use array_contains to check if agent_id is in the agent_ids JSON array
                filters["agent_ids"] = {"$contains": agent_id}
            else:
                logger.info("[SEARCH DEBUG] No agent_id filter - will search all documents (no access control)")

            logger.info(f"[SEARCH DEBUG] Final search filters: {filters}")

            # Perform the search with a tight timeout to prevent blocking
            logger.info("[SEARCH DEBUG] Calling similarity_search with:")
            logger.info(f"  - index_name: '{document_index}'")
            logger.info(f"  - endpoint_name: '{endpoint_name}'")
            logger.info(f"  - query_vector dimension: {len(query_embedding)}")
            logger.info(f"  - columns: {search_columns}")
            logger.info(f"  - filters: {filters}")
            logger.info(f"  - num_results: {limit}")

            try:
                search_results = await asyncio.wait_for(
                    index_repo.similarity_search(
                        index_name=document_index,
                        endpoint_name=endpoint_name,
                        query_vector=query_embedding,
                        columns=search_columns,
                        filters=filters,
                        num_results=limit,
                        user_token=user_token
                    ),
                    timeout=10
                )
                logger.info(f"[SEARCH DEBUG] âœ… Search completed successfully")
                logger.info(f"[SEARCH DEBUG] Raw results: {search_results}")
            except asyncio.TimeoutError:
                logger.warning("[SEARCH DEBUG] similarity_search timed out; returning empty results")
                return []
            except Exception as search_error:
                logger.error(f"[SEARCH DEBUG] âŒ Search failed with error: {search_error}", exc_info=True)
                return []

            # Extract data_array from REST API response
            # Repository returns: {'success': True, 'results': {'result': {'data_array': [...]}}}
            data_array = search_results.get('results', {}).get('result', {}).get('data_array', [])
            logger.info(f"[SEARCH DEBUG] Extracted {len(data_array)} results from data_array")

            if not data_array:
                logger.warning("[SEARCH DEBUG] âš ï¸ No results found!")
                logger.warning("[SEARCH DEBUG] Possible reasons:")
                logger.warning("  1. Group ID doesn't match documents")
                logger.warning("  2. Documents not properly indexed")
                logger.warning("  3. Vector index is empty or not accessible")
                return []

            # Get column positions for parsing results
            positions = DatabricksIndexSchemas.get_column_positions("document")
            logger.info(f"[SEARCH DEBUG] Column positions: {positions}")

            # Format results for return
            formatted_results = []
            logger.info(f"[SEARCH DEBUG] Formatting {len(data_array)} results...")

            for idx, result in enumerate(data_array):
                try:
                    logger.info(f"[SEARCH DEBUG] Processing result {idx + 1}:")
                    logger.info(f"  - Raw result type: {type(result)}")
                    logger.info(f"  - Raw result length: {len(result) if hasattr(result, '__len__') else 'N/A'}")

                    # Parse result based on schema positions
                    content = result[positions['content']] if 'content' in positions and len(result) > positions['content'] else ""
                    source = result[positions['source']] if 'source' in positions and len(result) > positions['source'] else ""
                    title = result[positions['title']] if 'title' in positions and len(result) > positions['title'] else ""
                    chunk_index = result[positions['chunk_index']] if 'chunk_index' in positions and len(result) > positions['chunk_index'] else 0

                    logger.info(f"  - Content preview: {content[:100]}..." if content else "  - No content")
                    logger.info(f"  - Source: {source}")
                    logger.info(f"  - Title: {title}")

                    # Get similarity score if available
                    score = result[-1] if len(result) > len(positions) else 0.0

                    formatted_result = {
                        "content": content,
                        "metadata": {
                            "source": source,
                            "title": title,
                            "chunk_index": chunk_index,
                            "score": score,
                            "group_id": group_id,
                            "execution_id": execution_id
                        }
                    }

                    formatted_results.append(formatted_result)

                    # Log the result for debugging
                    logger.info(f"[SEARCH DEBUG] Result {len(formatted_results)}:")
                    logger.info(f"  - Source: {source}")
                    logger.info(f"  - Score: {score:.4f}")
                    logger.info(f"  - Content preview: {content[:200]}..." if content else "  - No content")

                except Exception as e:
                    logger.error(f"Error formatting result: {e}")
                    continue

            logger.info(f"Returning {len(formatted_results)} formatted results")
            return formatted_results

        except Exception as e:
            logger.error(f"Error searching knowledge: {e}", exc_info=True)
            return []